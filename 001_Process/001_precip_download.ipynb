{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34dafd-e13f-419f-92e6-a58ef443cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g -f jup_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603a031-d084-4312-8dd0-75099eb02623",
   "metadata": {},
   "source": [
    "## Precipitation Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af85fd7-a0ba-4f1e-b420-130f630cb393",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precipitation Datasets\n",
    "gridMET = 'IDAHO_EPSCOR/GRIDMET',      variable = 'pr' (mm/day)\n",
    "DAYMET = 'NASA/ORNL/DAYMET_V4',        variable = 'prcp' (mm/day)\n",
    "NLDAS = 'NASA/NLDAS/FORA0125_H002',    variable = 'total_precipitation' (kg/m^2) hourly\n",
    "PRISM = 'OREGONSTATE/PRISM/AN81m',     variable = 'ppt'\t(mm/month)\n",
    "IMERG = 'NASA/GPM_L3/IMERG_MONTHLY_V06', variable = 'precipitation' (mm/hr) monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4023ae-e1fc-4419-a213-0ed06b3780af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=IM5cKQmb5igLL4-JAfiypca3YPsalcCmTKk3_jKfpIo&tc=FGZd-9GKIH8J2ZCt6iXLFFxVZ4hBkM3HiGLFqme8UME&cc=LmaBjQu3kzICdyLMAZxYToyrH1FT_7fcBeBSN3u889Y>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=IM5cKQmb5igLL4-JAfiypca3YPsalcCmTKk3_jKfpIo&tc=FGZd-9GKIH8J2ZCt6iXLFFxVZ4hBkM3HiGLFqme8UME&cc=LmaBjQu3kzICdyLMAZxYToyrH1FT_7fcBeBSN3u889Y</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1Adeu5BW1RYEvPfjohZtxb-TAj-_jQzxuLigpSqDNKo2hX4PcvfU3LZb92bM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "\n",
    "# # Uncomment to Authenticate and Initialize the Earth Engine Python API\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3c429-f63e-4b06-8ede-cefc38c6f141",
   "metadata": {},
   "source": [
    "## PRISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80eb3077-8df5-4e20-b7a6-5b2e05773a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of combinedFeatureCollection 114872\n",
      "PRISM_PRC_2016\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "\n",
    "# Uncomment to Authenticate and Initialize the Earth Engine Python API\n",
    "#ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "# Parameters\n",
    "filterdate = 2016\n",
    "startDate = ee.Date.fromYMD(filterdate, 1, 1)  # inclusive\n",
    "endDate = ee.Date.fromYMD(filterdate+1, 1, 1)  # exclusive\n",
    "# Feature Dataset\n",
    "combinedFeatureCollection = ee.FeatureCollection('projects/uom-project-0-1/assets/HPA_Irr_Croplands_2016')\n",
    "\n",
    "# Output some info\n",
    "print('Size of combinedFeatureCollection', combinedFeatureCollection.size().getInfo())\n",
    "\n",
    "# Subset of Polygons\n",
    "numberofPolygons = combinedFeatureCollection.size().getInfo()\n",
    "Sub_FeatureCollection = ee.FeatureCollection(combinedFeatureCollection.toList(numberofPolygons))\n",
    "\n",
    "# Precipitation Datasets\n",
    "prDatasets = [\n",
    "    'OREGONSTATE/PRISM/AN81m'\n",
    "            ]\n",
    "\n",
    "def extract_feature(feature, dataset):\n",
    "    geom = feature.geometry()\n",
    "\n",
    "    def extract_image(image):\n",
    "        image = image.unmask()\n",
    "        obj = image.reduceRegion(combinedreducers, geom, 30)  # 90m\n",
    "        return feature.setMulti(obj)\n",
    "\n",
    "    return dataset.filterBounds(geom).map(extract_image)\n",
    "\n",
    "combinedreducers = ee.Reducer.mean().combine(\n",
    "    reducer2=ee.Reducer.stdDev(),\n",
    "    sharedInputs=True\n",
    ").combine(\n",
    "    reducer2=ee.Reducer.sum(),\n",
    "    sharedInputs=True\n",
    ")\n",
    "\n",
    "for dataset in prDatasets:\n",
    "    # Precipitation dataset\n",
    "    pr = ee.ImageCollection(dataset) \\\n",
    "        .filterDate(startDate, endDate) \\\n",
    "        .select('ppt')\n",
    "\n",
    "    Sub_FeatureCollection_pr = Sub_FeatureCollection.map(lambda feature: extract_feature(feature, pr)).flatten()\n",
    "    Sub_FeatureCollection_pr = Sub_FeatureCollection_pr.select(['.*'], None, False)\n",
    "\n",
    "    # Output configuration\n",
    "    #link = str(startDate.split('-')[-3] + '_' + endDate.split('-')[-3])\n",
    "    description = dataset.split('/')[-2] + f'_PRC_{filterdate}'\n",
    "    output_folder = '001_Project_Exports'\n",
    "    print(description)\n",
    "\n",
    "    # Export as CSV to Google Drive\n",
    "    task = ee.batch.Export.table.toDrive(\n",
    "        collection=Sub_FeatureCollection_pr,\n",
    "        description=description,\n",
    "        folder=output_folder,\n",
    "        fileNamePrefix=description,\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3df0de-d983-4503-93b3-af4d9cdea432",
   "metadata": {},
   "source": [
    "## NLDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23823279-811d-48b8-bc82-ab3cd444222f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of combinedFeatureCollection 114872\n",
      "29\n",
      "NLDAS_PRC_Batch_1_2019\n",
      "NLDAS_PRC_Batch_2_2019\n",
      "NLDAS_PRC_Batch_3_2019\n",
      "NLDAS_PRC_Batch_4_2019\n",
      "NLDAS_PRC_Batch_5_2019\n",
      "NLDAS_PRC_Batch_6_2019\n",
      "NLDAS_PRC_Batch_7_2019\n",
      "NLDAS_PRC_Batch_8_2019\n",
      "NLDAS_PRC_Batch_9_2019\n",
      "NLDAS_PRC_Batch_10_2019\n",
      "NLDAS_PRC_Batch_11_2019\n",
      "NLDAS_PRC_Batch_12_2019\n",
      "NLDAS_PRC_Batch_13_2019\n",
      "NLDAS_PRC_Batch_14_2019\n",
      "NLDAS_PRC_Batch_15_2019\n",
      "NLDAS_PRC_Batch_16_2019\n",
      "NLDAS_PRC_Batch_17_2019\n",
      "NLDAS_PRC_Batch_18_2019\n",
      "NLDAS_PRC_Batch_19_2019\n",
      "NLDAS_PRC_Batch_20_2019\n",
      "NLDAS_PRC_Batch_21_2019\n",
      "NLDAS_PRC_Batch_22_2019\n",
      "NLDAS_PRC_Batch_23_2019\n",
      "NLDAS_PRC_Batch_24_2019\n",
      "NLDAS_PRC_Batch_25_2019\n",
      "NLDAS_PRC_Batch_26_2019\n",
      "NLDAS_PRC_Batch_27_2019\n",
      "NLDAS_PRC_Batch_28_2019\n",
      "NLDAS_PRC_Batch_29_2019\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "\n",
    "# Uncomment to Authenticate and Initialize the Earth Engine Python API\n",
    "#ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "# Parameters\n",
    "filterdate = 2019\n",
    "startDate = ee.Date.fromYMD(filterdate, 1, 1)  # inclusive\n",
    "endDate = ee.Date.fromYMD(filterdate+1, 1, 1)  # exclusive\n",
    "# Feature Dataset\n",
    "combinedFeatureCollection = ee.FeatureCollection('projects/uom-project-0-1/assets/HPA_Irr_Croplands_2019')\n",
    "\n",
    "## Output some info\n",
    "print('Size of combinedFeatureCollection', combinedFeatureCollection.size().getInfo())\n",
    "\n",
    "# Batch size for processing polygons\n",
    "batch_size = 4000  # Adjust this value as needed\n",
    "\n",
    "# Get the total number of polygons\n",
    "total_polygons = combinedFeatureCollection.size().getInfo()\n",
    "\n",
    "# Calculate the number of batches required\n",
    "num_batches = total_polygons // batch_size + (1 if total_polygons % batch_size != 0 else 0)\n",
    "print(num_batches)\n",
    "\n",
    "# Create a nested list of feature collections\n",
    "feature_collection_list = ee.List.sequence(0, total_polygons, batch_size).map(\n",
    "    lambda start_index: combinedFeatureCollection.toList(batch_size, start_index)\n",
    ")\n",
    "\n",
    "# Precipitation Datasets\n",
    "prDatasets = [\n",
    "    'NASA/NLDAS/FORA0125_H002'\n",
    "]\n",
    "\n",
    "# Process polygons in batches\n",
    "for i in range(num_batches):\n",
    "    Sub_FeatureCollection = ee.FeatureCollection(ee.List(feature_collection_list.get(i)).flatten())\n",
    "    #print(Sub_FeatureCollection, f\"Sub_FeatureCollection (Batch {i+1}/{num_batches})\")\n",
    "\n",
    "    def extract_feature(feature, dataset):\n",
    "        geom = feature.geometry()\n",
    "\n",
    "        def extract_image(image):\n",
    "            image = image.unmask()\n",
    "            obj = image.reduceRegion(combinedreducers, geom, 30)  # 90m\n",
    "            return feature.setMulti(obj)\n",
    "\n",
    "        return dataset.filterBounds(geom).map(extract_image)\n",
    "\n",
    "    combinedreducers = ee.Reducer.mean().combine(\n",
    "        reducer2=ee.Reducer.stdDev(),\n",
    "        sharedInputs=True\n",
    "    ).combine(\n",
    "        reducer2=ee.Reducer.max(),\n",
    "        sharedInputs=True\n",
    "    )\n",
    "\n",
    "    for dataset in prDatasets:\n",
    "        # Precipitation dataset\n",
    "        pr = ee.ImageCollection(dataset) \\\n",
    "            .filterDate(startDate, endDate) \\\n",
    "            .select('total_precipitation')\n",
    "        \n",
    "        # Calculate dailyDataset using the provided function\n",
    "        difdate = endDate.advance(-1, 'day').difference(startDate, 'day')\n",
    "        createList = ee.List.sequence(0, difdate)\n",
    "        listdates = createList.map(lambda day: startDate.advance(day, 'day'))\n",
    "\n",
    "        # Daily precipitation dataset\n",
    "        dailyDataset = ee.ImageCollection.fromImages(listdates.map(lambda summarize_day:\n",
    "            pr.filterDate(ee.Date(summarize_day), ee.Date(summarize_day).advance(1, 'day'))\n",
    "              .sum().select(['total_precipitation'])\n",
    "              .copyProperties(pr.first()).setMulti({\n",
    "                    'Date': ee.Date(summarize_day),\n",
    "                    'system:time_start': ee.Date(summarize_day).millis()\n",
    "                })\n",
    "        ))\n",
    "\n",
    "        Sub_FeatureCollection_pr = Sub_FeatureCollection.map(lambda feature: extract_feature(feature, dailyDataset)).flatten()\n",
    "        Sub_FeatureCollection_pr = Sub_FeatureCollection_pr.select(['.*'], None, False)\n",
    "\n",
    "        # Output configuration\n",
    "        #link = str(startDate.split('-')[-3] + '_' + endDate.split('-')[-3])\n",
    "        description = f'NLDAS_PRC_Batch_{i+1}_{filterdate}'\n",
    "        output_folder = '001_Project_Exports'\n",
    "        print(description)\n",
    "\n",
    "        # Export as CSV to Google Drive\n",
    "        task = ee.batch.Export.table.toDrive(\n",
    "            collection=Sub_FeatureCollection_pr,\n",
    "            description=description,\n",
    "            folder=output_folder,\n",
    "            fileNamePrefix=description,\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        task.start()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e69257-2701-43f2-8e7d-89d6eb225e1a",
   "metadata": {},
   "source": [
    "## returns month and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a44088f-628f-417d-86eb-1f0d36815fa8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of combinedFeatureCollection 129482\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "\n",
    "# Initialize the Earth Engine Python API\n",
    "ee.Initialize()\n",
    "\n",
    "# Parameters\n",
    "filterdate = 2016\n",
    "startDate = ee.Date.fromYMD(filterdate, 1, 1)  # inclusive\n",
    "endDate = ee.Date.fromYMD(filterdate+1, 1, 1)  # exclusive\n",
    "\n",
    "# Feature Dataset\n",
    "combinedFeatureCollection = ee.FeatureCollection('projects/uom-project-0-1/assets/HPA_Irr_Croplands_2016')\n",
    "\n",
    "# Output some info\n",
    "print('Size of combinedFeatureCollection', combinedFeatureCollection.size().getInfo())\n",
    "\n",
    "# Subset of Polygons\n",
    "numberofPolygons = 2\n",
    "Sub_FeatureCollection = ee.FeatureCollection(combinedFeatureCollection.toList(numberofPolygons))\n",
    "#print(Sub_FeatureCollection, \"Sub_FeatureCollection\")\n",
    "\n",
    "# Precipitation Dataset\n",
    "pr = ee.ImageCollection('NASA/ORNL/DAYMET_V4') \\\n",
    "    .filterDate(startDate, endDate) \\\n",
    "    .select('prcp')\n",
    "\n",
    "# Do some quick checks\n",
    "#print('Size of Precipitation Collection', pr.size())\n",
    "#print(pr.first(), \"example precipitation image\")\n",
    "\n",
    "# Extract Precipitation values for each location for csv output\n",
    "def extract_feature(feature):\n",
    "    geom = feature.geometry()\n",
    "\n",
    "    def extract_image(image):\n",
    "        image = image.unmask()\n",
    "        date = ee.Date(image.get('system:time_start'))\n",
    "        year = date.get('year')\n",
    "        month = date.get('month')\n",
    "        obj = image.reduceRegion(ee.Reducer.sum(), geom, 30)  # 90m\n",
    "        return feature.setMulti(obj).set('year', year).set('month', month)\n",
    "\n",
    "    return pr.filterBounds(geom).map(extract_image)\n",
    "\n",
    "Sub_FeatureCollection_pr = Sub_FeatureCollection.map(extract_feature).flatten()\n",
    "\n",
    "# # Update system:index column\n",
    "Sub_FeatureCollection_pr = Sub_FeatureCollection_pr.map(lambda feature: feature.set('system:index', ee.String(feature.get('system:index')).split('_').get(1)))\n",
    "\n",
    "# #print(Sub_FeatureCollection_pr.first(), \"Sub_FeatureCollection_pr Example\")\n",
    "\n",
    "# Monthly Total Precipitation\n",
    "monthlyPrecipitation = Sub_FeatureCollection_pr.reduceColumns(ee.Reducer.sum().group(1, 'monthly_total'), ['month', 'prcp'])\n",
    "\n",
    "# Output configuration\n",
    "link = str(filterdate)\n",
    "description = 'DAYMET_Prcp_1to5_2016'\n",
    "output_folder = '001_Project_Exports'\n",
    "\n",
    "# Export as CSV to Google Drive\n",
    "task = ee.batch.Export.table.toDrive(\n",
    "    collection=Sub_FeatureCollection_pr,\n",
    "    description=description,\n",
    "    folder=output_folder,\n",
    "    fileNamePrefix=description,\n",
    "    fileFormat='CSV'\n",
    ")\n",
    "task.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea85b677-53a2-4349-999f-7e53d8665c4c",
   "metadata": {},
   "source": [
    "## Batch downloading for gridMET and DAYMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba998f44-cefe-403b-9394-c998a66d62c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of combinedFeatureCollection 114872\n",
      "29\n",
      "DAYMET_PRC_Batch_1_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_2_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_3_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_4_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_5_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_6_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_7_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_8_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_9_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_10_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_11_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_12_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_13_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_14_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_15_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_16_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_17_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_18_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_19_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_20_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_21_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_22_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_23_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_24_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_25_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_26_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_27_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_28_2016\n",
      "Done!\n",
      "DAYMET_PRC_Batch_29_2016\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "\n",
    "# Uncomment to Authenticate and Initialize the Earth Engine Python API\n",
    "#ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "# Parameters\n",
    "filterdate = 2016\n",
    "startDate = ee.Date.fromYMD(filterdate, 1, 1)  # inclusive\n",
    "endDate = ee.Date.fromYMD(filterdate+1, 1, 1)  # exclusive\n",
    "# Feature Dataset\n",
    "combinedFeatureCollection = ee.FeatureCollection('projects/uom-project-0-1/assets/HPA_Irr_Croplands_2016')\n",
    "\n",
    "## Output some info\n",
    "print('Size of combinedFeatureCollection', combinedFeatureCollection.size().getInfo())\n",
    "\n",
    "# Batch size for processing polygons\n",
    "batch_size = 4000  # Adjust this value as needed\n",
    "\n",
    "# Get the total number of polygons\n",
    "total_polygons = combinedFeatureCollection.size().getInfo()\n",
    "\n",
    "# Calculate the number of batches required\n",
    "num_batches = total_polygons // batch_size + (1 if total_polygons % batch_size != 0 else 0)\n",
    "print(num_batches)\n",
    "\n",
    "# Create a nested list of feature collections\n",
    "feature_collection_list = ee.List.sequence(0, total_polygons, batch_size).map(\n",
    "    lambda start_index: combinedFeatureCollection.toList(batch_size, start_index)\n",
    ")\n",
    "\n",
    "# Precipitation Datasets\n",
    "prDatasets = [\n",
    "    'NASA/ORNL/DAYMET_V4'\n",
    "    ]\n",
    "\n",
    "# Process polygons in batches\n",
    "for i in range(num_batches):\n",
    "    Sub_FeatureCollection = ee.FeatureCollection(ee.List(feature_collection_list.get(i)).flatten())\n",
    "    #print(Sub_FeatureCollection, f\"Sub_FeatureCollection (Batch {i+1}/{num_batches})\")\n",
    "\n",
    "    def extract_feature(feature, dataset):\n",
    "        geom = feature.geometry()\n",
    "\n",
    "        def extract_image(image):\n",
    "            image = image.unmask()\n",
    "            obj = image.reduceRegion(combinedreducers, geom, 30)  # 90m\n",
    "            return feature.setMulti(obj)\n",
    "\n",
    "        return dataset.filterBounds(geom).map(extract_image)\n",
    "\n",
    "    combinedreducers = ee.Reducer.mean().combine(\n",
    "        reducer2=ee.Reducer.stdDev(),\n",
    "        sharedInputs=True\n",
    "    ).combine(\n",
    "        reducer2=ee.Reducer.max(),\n",
    "        sharedInputs=True\n",
    "    )\n",
    "\n",
    "    for dataset in prDatasets:\n",
    "        # Precipitation dataset\n",
    "        pr = ee.ImageCollection(dataset) \\\n",
    "            .filterDate(startDate, endDate) \\\n",
    "            .select('prcp')\n",
    "\n",
    "        Sub_FeatureCollection_pr = Sub_FeatureCollection.map(lambda feature: extract_feature(feature, pr)).flatten()\n",
    "        Sub_FeatureCollection_pr = Sub_FeatureCollection_pr.select(['.*'], None, False)\n",
    "\n",
    "        # Output configuration\n",
    "        #link = str(startDate.split('-')[-3] + '_' + endDate.split('-')[-3])\n",
    "        description = f'DAYMET_PRC_Batch_{i+1}_{filterdate}'\n",
    "        output_folder = '001_Project_Exports'\n",
    "        print(description)\n",
    "\n",
    "        # Export as CSV to Google Drive\n",
    "        task = ee.batch.Export.table.toDrive(\n",
    "            collection=Sub_FeatureCollection_pr,\n",
    "            description=description,\n",
    "            folder=output_folder,\n",
    "            fileNamePrefix=description,\n",
    "            fileFormat='CSV'\n",
    "        )\n",
    "        task.start()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29103c05-55b3-49a5-bf62-7c41327ac077",
   "metadata": {},
   "source": [
    "## IMERG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06042dc9-09cb-4b46-9e5f-3126fa2f2a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of combinedFeatureCollection 114872\n",
      "IMERG_MONTHLY_V06_PRC_2021\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "\n",
    "# Uncomment to Authenticate and Initialize the Earth Engine Python API\n",
    "#ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "# Parameters\n",
    "filterdate = 2021\n",
    "startDate = ee.Date.fromYMD(filterdate, 1, 1)  # inclusive\n",
    "endDate = ee.Date.fromYMD(filterdate+1, 1, 1)  # exclusive\n",
    "# Feature Dataset\n",
    "combinedFeatureCollection = ee.FeatureCollection('projects/uom-project-0-1/assets/HPA_Irr_Croplands_2021')\n",
    "\n",
    "# Output some info\n",
    "print('Size of combinedFeatureCollection', combinedFeatureCollection.size().getInfo())\n",
    "\n",
    "# Subset of Polygons\n",
    "numberofPolygons = combinedFeatureCollection.size().getInfo()\n",
    "Sub_FeatureCollection = ee.FeatureCollection(combinedFeatureCollection.toList(numberofPolygons))\n",
    "\n",
    "# Precipitation Dataset\n",
    "prDatasets = [\n",
    "    'NASA/GPM_L3/IMERG_MONTHLY_V06'\n",
    "]\n",
    "\n",
    "# below function return number of days in a month\n",
    "def getDaysInMonth(y, m):\n",
    "    dt = ee.Date.fromYMD(y, m, 1)\n",
    "    n = dt.advance(1, \"month\").difference(dt, 'day')\n",
    "    return n\n",
    "\n",
    "# below function will convert mm/hr to mm/month for the GPM data\n",
    "def monthly(image):\n",
    "    dt = ee.Date(image.get(\"system:time_end\"))\n",
    "    y = dt.get('year')\n",
    "    m = dt.get('month')\n",
    "    days = getDaysInMonth(y, m)\n",
    "    return image.multiply(days).multiply(24).set({\n",
    "        \"system:time_start\": image.get(\"system:time_start\"),\n",
    "        \"system:time_end\": image.get(\"system:time_end\")\n",
    "    })\n",
    "\n",
    "def extract_feature(feature, dataset):\n",
    "    geom = feature.geometry()\n",
    "\n",
    "    def extract_image(image):\n",
    "        image = image.unmask()\n",
    "        obj = image.reduceRegion(combinedreducers, geom, 30)  # 90m\n",
    "        return feature.setMulti(obj)\n",
    "\n",
    "    return dataset.filterBounds(geom).map(extract_image)\n",
    "\n",
    "combinedreducers = ee.Reducer.mean().combine(\n",
    "    reducer2=ee.Reducer.stdDev(),\n",
    "    sharedInputs=True\n",
    ").combine(\n",
    "    reducer2=ee.Reducer.sum(),\n",
    "    sharedInputs=True\n",
    ")\n",
    "\n",
    "for dataset in prDatasets:\n",
    "    # Precipitation dataset\n",
    "    pr = ee.ImageCollection(dataset) \\\n",
    "        .filterDate(startDate, endDate) \\\n",
    "        .select('precipitation')\n",
    "\n",
    "    pr_monthly = pr.map(monthly)  # Applying the monthly function\n",
    "\n",
    "    Sub_FeatureCollection_pr = Sub_FeatureCollection.map(lambda feature: extract_feature(feature, pr_monthly)).flatten()\n",
    "    Sub_FeatureCollection_pr = Sub_FeatureCollection_pr.select(['.*'], None, False)\n",
    "\n",
    "    # Output configuration\n",
    "    description = dataset.split('/')[-1] + f'_PRC_{filterdate}'\n",
    "    output_folder = '001_Project_Exports'\n",
    "    print(description)\n",
    "\n",
    "    # Export as CSV to Google Drive\n",
    "    task = ee.batch.Export.table.toDrive(\n",
    "        collection=Sub_FeatureCollection_pr,\n",
    "        description=description,\n",
    "        folder=output_folder,\n",
    "        fileNamePrefix=description,\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be94756d-c857-4805-9d99-591b7c5d5cfd",
   "metadata": {},
   "source": [
    "## Download only one batch NLDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "629029c8-3634-4ba2-8b72-4cf3d37f23bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "NLDAS_PRC_Batch_9_2019\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "\n",
    "# Uncomment to Authenticate and Initialize the Earth Engine Python API\n",
    "# ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "# Parameters\n",
    "filterdate = 2019\n",
    "startDate = ee.Date.fromYMD(filterdate, 1, 1)  # inclusive\n",
    "endDate = ee.Date.fromYMD(filterdate+1, 1, 1)  # exclusive\n",
    "# Feature Dataset\n",
    "combinedFeatureCollection = ee.FeatureCollection('projects/uom-project-0-1/assets/HPA_Irr_Croplands_2019')\n",
    "\n",
    "# Batch size for processing polygons\n",
    "batch_size = 4000  # Adjust this value as needed\n",
    "\n",
    "# Get the total number of polygons\n",
    "total_polygons = combinedFeatureCollection.size().getInfo()\n",
    "\n",
    "# Calculate the number of batches required\n",
    "num_batches = total_polygons // batch_size + (1 if total_polygons % batch_size != 0 else 0)\n",
    "print(num_batches)\n",
    "\n",
    "# Specify the batch you want to process (0-indexed)\n",
    "target_batch = 9\n",
    "\n",
    "# Create a nested list of feature collections\n",
    "feature_collection_list = ee.List.sequence(0, total_polygons, batch_size).map(\n",
    "    lambda start_index: combinedFeatureCollection.toList(batch_size, start_index)\n",
    ")\n",
    "\n",
    "# Precipitation Datasets\n",
    "prDatasets = [\n",
    "    'NASA/NLDAS/FORA0125_H002'\n",
    "]\n",
    "\n",
    "# Process only batch 9\n",
    "for i in range(num_batches):\n",
    "    if i == target_batch:\n",
    "        Sub_FeatureCollection = ee.FeatureCollection(ee.List(feature_collection_list.get(i)).flatten())\n",
    "        \n",
    "        # Define the extract_feature function and combinedreducers here\n",
    "        \n",
    "        for dataset in prDatasets:\n",
    "            # Precipitation dataset\n",
    "            pr = ee.ImageCollection(dataset) \\\n",
    "                .filterDate(startDate, endDate) \\\n",
    "                .select('total_precipitation')\n",
    "            \n",
    "            # Calculate dailyDataset using the provided function\n",
    "            difdate = endDate.advance(-1, 'day').difference(startDate, 'day')\n",
    "            createList = ee.List.sequence(0, difdate)\n",
    "            listdates = createList.map(lambda day: startDate.advance(day, 'day'))\n",
    "            \n",
    "            # Daily precipitation dataset\n",
    "            dailyDataset = ee.ImageCollection.fromImages(listdates.map(lambda summarize_day:\n",
    "                pr.filterDate(ee.Date(summarize_day), ee.Date(summarize_day).advance(1, 'day'))\n",
    "                  .sum().select(['total_precipitation'])\n",
    "                  .copyProperties(pr.first()).setMulti({\n",
    "                        'Date': ee.Date(summarize_day),\n",
    "                        'system:time_start': ee.Date(summarize_day).millis()\n",
    "                    })\n",
    "            ))\n",
    "\n",
    "            Sub_FeatureCollection_pr = Sub_FeatureCollection.map(lambda feature: extract_feature(feature, dailyDataset)).flatten()\n",
    "            Sub_FeatureCollection_pr = Sub_FeatureCollection_pr.select(['.*'], None, False)\n",
    "\n",
    "            # Output configuration\n",
    "            description = f'NLDAS_PRC_Batch_{i}_{filterdate}'\n",
    "            output_folder = '001_Project_Exports'\n",
    "            print(description)\n",
    "\n",
    "            # Export as CSV to Google Drive\n",
    "            task = ee.batch.Export.table.toDrive(\n",
    "                collection=Sub_FeatureCollection_pr,\n",
    "                description=description,\n",
    "                folder=output_folder,\n",
    "                fileNamePrefix=description,\n",
    "                fileFormat='CSV'\n",
    "            )\n",
    "            task.start()\n",
    "\n",
    "        break  # Exit the loop after processing the target batch\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5bcd90-45ba-4607-aa17-6a53cf26411f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Geospatial",
   "language": "python",
   "name": "geospatial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
